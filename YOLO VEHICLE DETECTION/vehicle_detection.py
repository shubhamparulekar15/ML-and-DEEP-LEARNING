# -*- coding: utf-8 -*-
"""vehicle_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tdjj71n8HjQx1Qgrlc8RSwfEhJWj6HPX
"""

import numpy as np
import time
import cv2
import os


HOME_PATH = "./"

labelsPath = HOME_PATH+"coco.names"
LABELS = open(labelsPath).read().strip().split("\n")
print(LABELS)

weightsPath = HOME_PATH+"yolov3.weights"
configPath = HOME_PATH+"yolov3.cfg"

net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)




vidcap = cv2.VideoCapture('test_video_1.mp4')

success,image = vidcap.read()

img_array = []
while True:

	success,image = vidcap.read()



	if success:
		# image = cv2.imread(HOME_PATH+"test_1.jpg")
		(H, W) = image.shape[:2]
		size=(W,H)
		# determine only the *output* layer names that we need from YOLO
		ln = net.getLayerNames()
		ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]
		# construct a blob from the input image and then perform a forward
		# pass of the YOLO object detector, giving us our bounding boxes and
		# associated probabilities
		blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),
			swapRB=True, crop=False)
		net.setInput(blob)
		start = time.time()
		layerOutputs = net.forward(ln)
		end = time.time()
		# show timing information on YOLO
		print("[INFO] YOLO took {:.6f} seconds".format(end - start))

		layerOutputs[0]

		boxes = []
		confidences = []
		classIDs = []

		for output in layerOutputs:
			# loop over each of the detections
			for detection in output:
				# extract the class ID and confidence (i.e., probability) of
				# the current object detection
				scores = detection[5:]
				classID = np.argmax(scores)
				confidence = scores[classID]
				# print(scores,classID,confidence)
				# filter out weak predictions by ensuring the detected
				# probability is greater than the minimum probability
				if confidence > 0.7:
					# scale the bounding box coordinates back relative to the
					# size of the image, keeping in mind that YOLO actually
					# returns the center (x, y)-coordinates of the bounding
					# box followed by the boxes' width and height
					box = detection[0:4] * np.array([W, H, W, H])
					(centerX, centerY, width, height) = box.astype("int")
					# use the center (x, y)-coordinates to derive the top and
					# and left corner of the bounding box
					x = int(centerX - (width / 2))
					y = int(centerY - (height / 2))
					# update our list of bounding box coordinates, confidences,
					# and class IDs
					boxes.append([x, y, int(width), int(height)])
					confidences.append(float(confidence))
					classIDs.append(classID)

		idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.7,
			0.5)

		print(boxes)


		if len(idxs) > 0:
			# loop over the indexes we are keeping
			for i in idxs.flatten():
				# extract the bounding box coordinates
				(x, y) = (boxes[i][0], boxes[i][1])
				(w, h) = (boxes[i][2], boxes[i][3])
				# draw a bounding box rectangle and label on the image
				# color = [int(c) for c in COLORS[classIDs[i]]]
				cv2.rectangle(image, (x, y), (x + w, y + h), (0,255,0), 2)
				# text = "{}: {:.4f}".format(LABELS[classIDs[i]], confidences[i])
				# cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,
				# 	0.5, color, 2)
		# show the output image
		img_array.append(image)
		# cv2.imshow("0", image)
		# cv2.waitKey(1)		
	else:
		break


out = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)
 
for i in range(len(img_array)):
    out.write(img_array[i])
out.release()